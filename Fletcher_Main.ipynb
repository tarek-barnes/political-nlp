{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import NMF, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "articles1 = pd.read_csv(\"articles1.csv\")\n",
    "articles2 = pd.read_csv(\"articles2.csv\")\n",
    "articles3 = pd.read_csv(\"articles3.csv\")\n",
    "articles = pd.concat([articles1, articles2, articles3])\n",
    "\n",
    "political_score_dict = {\"Atlantic\": -1,\n",
    "                        \"Breitbart\": 1,\n",
    "                        \"Business Insider\": -1,\n",
    "                        \"Buzzfeed News\": -1,\n",
    "                        \"CNN\": -1,\n",
    "                        \"Fox News\": 1,\n",
    "                        \"Guardian\": -1,\n",
    "                        \"NPR\": -1,\n",
    "                        \"National Review\": 1,\n",
    "                        \"New York Post\": 1,\n",
    "                        \"New York Times\": -1,\n",
    "                        \"Reuters\": -1,\n",
    "                        \"Talking Points Memo\": -1,\n",
    "                        \"Vox\": -1,\n",
    "                        \"Washington Post\": -1}\n",
    "\n",
    "articles['score'] = articles['publication'].apply(lambda x: political_score_dict[x])\n",
    "articles.head()\n",
    "\n",
    "# This helps to cut down the volume of data I'm working with\n",
    "articles_train, articles_test = train_test_split(articles, test_size=0.5)\n",
    "\n",
    "uci = pd.read_csv(\"uci-news-aggregator.csv\")\n",
    "uci.columns = [k.lower() for k in uci.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pub in set(articles.publication):\n",
    "    sub_df = articles[articles.publication == pub]\n",
    "    print(pub)\n",
    "    print(len(sub_df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28337</th>\n",
       "      <td>81796</td>\n",
       "      <td>123240</td>\n",
       "      <td>Model sues ex-sugar daddy for kicking her out ...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>Julia Marsh and Laura Italiano</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://web.archive.org/web/20170115000208/htt...</td>\n",
       "      <td>A foxy Russian model is suing a married   deve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14897</th>\n",
       "      <td>68305</td>\n",
       "      <td>96125</td>\n",
       "      <td>Harry Reid To Republicans: You Better Not Bloc...</td>\n",
       "      <td>Talking Points Memo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-02-13</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://web.archive.org/web/20160214013155/htt...</td>\n",
       "      <td>Senate Minority Leader Harry Reid ( ) released...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39140</th>\n",
       "      <td>142602</td>\n",
       "      <td>213716</td>\n",
       "      <td>How Monday’s train attack by a 17-year-old Afg...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Rick Noack</td>\n",
       "      <td>2016-07-19</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>https://web.archive.org/web/20160720000114/htt...</td>\n",
       "      <td>Germany has so far been spared a   Isla...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38038</th>\n",
       "      <td>91497</td>\n",
       "      <td>136413</td>\n",
       "      <td>She took down a mugger with MMA, then audition...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>Carl Stroud, The Sun</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>http://nypost.com/2016/06/02/she-took-down-a-m...</td>\n",
       "      <td>A beauty queen in high heels fought off a mugg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>3973</td>\n",
       "      <td>21691</td>\n",
       "      <td>N.C.A.A. Moves Championship Events From North ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Marc Tracy and Alan Blinder</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The N. C. A. A. responding to a contentious No...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0      id                                              title  \\\n",
       "28337       81796  123240  Model sues ex-sugar daddy for kicking her out ...   \n",
       "14897       68305   96125  Harry Reid To Republicans: You Better Not Bloc...   \n",
       "39140      142602  213716  How Monday’s train attack by a 17-year-old Afg...   \n",
       "38038       91497  136413  She took down a mugger with MMA, then audition...   \n",
       "3973         3973   21691  N.C.A.A. Moves Championship Events From North ...   \n",
       "\n",
       "               publication                          author        date  \\\n",
       "28337        New York Post  Julia Marsh and Laura Italiano  2017-01-13   \n",
       "14897  Talking Points Memo                             NaN  2016-02-13   \n",
       "39140      Washington Post                      Rick Noack  2016-07-19   \n",
       "38038        New York Post            Carl Stroud, The Sun  2016-06-02   \n",
       "3973        New York Times     Marc Tracy and Alan Blinder  2017-03-08   \n",
       "\n",
       "         year  month                                                url  \\\n",
       "28337  2017.0    1.0  https://web.archive.org/web/20170115000208/htt...   \n",
       "14897  2016.0    2.0  https://web.archive.org/web/20160214013155/htt...   \n",
       "39140  2016.0    7.0  https://web.archive.org/web/20160720000114/htt...   \n",
       "38038  2016.0    6.0  http://nypost.com/2016/06/02/she-took-down-a-m...   \n",
       "3973   2017.0    3.0                                                NaN   \n",
       "\n",
       "                                                 content  score  \n",
       "28337  A foxy Russian model is suing a married   deve...      1  \n",
       "14897  Senate Minority Leader Harry Reid ( ) released...     -1  \n",
       "39140         Germany has so far been spared a   Isla...     -1  \n",
       "38038  A beauty queen in high heels fought off a mugg...      1  \n",
       "3973   The N. C. A. A. responding to a contentious No...     -1  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Do the topics seem different / are they discussing different things?\n",
    "\n",
    "###  Do topic modeling for each publication segment, compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pub_dict():\n",
    "    pub_dict = {}\n",
    "\n",
    "    for pub in set(articles_train.publication):\n",
    "        pub_df = articles_train[articles_train.publication == pub]\n",
    "        pub_data = zip(pub_df.date, pub_df.title, pub_df.content)\n",
    "        pub_dict[pub] = pub_data\n",
    "        \n",
    "    return pub_dict    \n",
    "\n",
    "\n",
    "def get_topics(model, feature_names, no_top_words):\n",
    "    topics = []\n",
    "    for _, topic in enumerate(model.components_):\n",
    "        topics.append([(feature_names[i], np.round(lsa_cv.components_[_][i], 3)) for i in topic.argsort()[:-no_top_words-1:-1]])\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3),  \n",
    "                                   stop_words='english', \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   min_df = 0.05,\n",
    "                                   max_df = 0.6)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3),  \n",
    "                                   stop_words='english', \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   min_df = 0.05,\n",
    "                                   max_df = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business Insider': <zip at 0x1a0dd4cf48>,\n",
       " 'CNN': <zip at 0x1a0dd559c8>,\n",
       " 'Fox News': <zip at 0x1a0dd55488>,\n",
       " 'Vox': <zip at 0x1a0dd55088>,\n",
       " 'Talking Points Memo': <zip at 0x1a0dd553c8>,\n",
       " 'Buzzfeed News': <zip at 0x1a0dd55a88>,\n",
       " 'New York Times': <zip at 0x1a0dd55f88>,\n",
       " 'National Review': <zip at 0x1a0dd4ca48>,\n",
       " 'Guardian': <zip at 0x1a0dd61408>,\n",
       " 'Atlantic': <zip at 0x1a0dd61208>,\n",
       " 'NPR': <zip at 0x1a0dd614c8>,\n",
       " 'Breitbart': <zip at 0x1a0dd55f48>,\n",
       " 'Washington Post': <zip at 0x1a0dd66dc8>,\n",
       " 'New York Post': <zip at 0x1a0dd4c088>,\n",
       " 'Reuters': <zip at 0x1a0dd66fc8>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 10\n",
    "lsa_tfidf = TruncatedSVD(n_components=n_comp)\n",
    "lsa_cv = TruncatedSVD(n_components=n_comp)\n",
    "nmf_cv = NMF(n_components=n_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "pub_dict = get_pub_dict()\n",
    "publications = pub_dict.keys()\n",
    "\n",
    "topic_pub_dict = {}\n",
    "\n",
    "for pub in publications:\n",
    "    pub_articles = [k[2] for k in pub_dict[pub]]\n",
    "    \n",
    "    cv_articles_data = count_vectorizer.fit_transform(pub_articles)\n",
    "    tfidf_articles_data = tfidf_vectorizer.fit_transform(pub_articles)\n",
    "    \n",
    "    lsa_tfidf_articles_data = lsa_tfidf.fit_transform(tfidf_articles_data)\n",
    "    lsa_cv_articles_data = lsa_cv.fit_transform(cv_articles_data)\n",
    "    nmf_cv_articles_data = nmf_cv.fit_transform(cv_articles_data)\n",
    "    \n",
    "    pub_lsa_tfidf = get_topics(lsa_tfidf, tfidf_vectorizer.get_feature_names(), num_top_words)\n",
    "    pub_lsa_cv    = get_topics(lsa_cv,    count_vectorizer.get_feature_names(), num_top_words)\n",
    "    pub_nmf_cv    = get_topics(nmf_cv,    count_vectorizer.get_feature_names(), num_top_words)\n",
    "    \n",
    "    topic_pub_dict[pub] = [pub_lsa_tfidf, pub_lsa_cv, pub_nmf_cv]\n",
    "    \n",
    "    \n",
    "pub_df = pd.DataFrame.from_dict(topic_pub_dict, orient=\"index\")   \n",
    "pub_df.columns = [\"lsa_tfidf\", \"lsa_cv\", \"nmf_cv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lsa_tfidf</th>\n",
       "      <th>lsa_cv</th>\n",
       "      <th>nmf_cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>New York Times</th>\n",
       "      <td>[[(trump, 0.586), (mr trump, 0.366), (ms, 0.10...</td>\n",
       "      <td>[[(trump, 0.586), (mr trump, 0.366), (presiden...</td>\n",
       "      <td>[[(trump, 0.586), (mr trump, 0.366), (presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Post</th>\n",
       "      <td>[[(trump, 0.193), (new, 0.25), (says, 0.185), ...</td>\n",
       "      <td>[[(new, 0.25), (like, 0.236), (just, 0.229), (...</td>\n",
       "      <td>[[(game, 0.09), (season, 0.081), (team, 0.082)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>National Review</th>\n",
       "      <td>[[(trump, 0.6), (clinton, 0.171), (obama, 0.14...</td>\n",
       "      <td>[[(trump, 0.6), (clinton, 0.171), (president, ...</td>\n",
       "      <td>[[(trump, 0.6), (donald, 0.088), (donald trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reuters</th>\n",
       "      <td>[[(trump, 0.422), (percent, 0.217), (billion, ...</td>\n",
       "      <td>[[(trump, 0.422), (percent, 0.217), (president...</td>\n",
       "      <td>[[(trump, 0.422), (president, 0.188), (campaig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPR</th>\n",
       "      <td>[[(trump, 0.361), (said, 0.322), (president, 0...</td>\n",
       "      <td>[[(trump, 0.361), (said, 0.322), (president, 0...</td>\n",
       "      <td>[[(think, 0.144), (know, 0.126), (going, 0.135...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Insider</th>\n",
       "      <td>[[(trump, 0.414), (said, 0.393), (president, 0...</td>\n",
       "      <td>[[(trump, 0.414), (said, 0.393), (people, 0.26...</td>\n",
       "      <td>[[(people, 0.265), (think, 0.17), (like, 0.213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talking Points Memo</th>\n",
       "      <td>[[(trump, 0.692), (president, 0.206), (house, ...</td>\n",
       "      <td>[[(trump, 0.692), (president, 0.206), (people,...</td>\n",
       "      <td>[[(trump, 0.692), (donald, 0.087), (donald tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>[[(trump, 0.609), (president, 0.186), (people,...</td>\n",
       "      <td>[[(trump, 0.609), (people, 0.197), (president,...</td>\n",
       "      <td>[[(trump, 0.609), (campaign, 0.11), (donald, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buzzfeed News</th>\n",
       "      <td>[[(trump, 0.212), (like, 0.253), (buzzfeed, 0....</td>\n",
       "      <td>[[(like, 0.253), (trump, 0.212), (just, 0.196)...</td>\n",
       "      <td>[[(like, 0.253), (just, 0.196), (says, 0.093),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guardian</th>\n",
       "      <td>[[(trump, 0.415), (like, 0.192), (new, 0.176),...</td>\n",
       "      <td>[[(trump, 0.415), (like, 0.192), (just, 0.179)...</td>\n",
       "      <td>[[(like, 0.192), (just, 0.179), (don, 0.114), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington Post</th>\n",
       "      <td>[[(trump, 0.601), (clinton, 0.117), (president...</td>\n",
       "      <td>[[(trump, 0.601), (president, 0.174), (know, 0...</td>\n",
       "      <td>[[(trump, 0.601), (donald, 0.079), (donald tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>[[(trump, 0.625), (clinton, 0.145), (president...</td>\n",
       "      <td>[[(trump, 0.625), (clinton, 0.145), (president...</td>\n",
       "      <td>[[(trump, 0.625), (campaign, 0.1), (donald, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breitbart</th>\n",
       "      <td>[[(trump, 0.633), (clinton, 0.242), (president...</td>\n",
       "      <td>[[(trump, 0.633), (says, 0.457), (clinton, 0.2...</td>\n",
       "      <td>[[(trump, 0.633), (donald, 0.094), (donald tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fox News</th>\n",
       "      <td>[[(trump, 0.548), (clinton, 0.417), (campaign,...</td>\n",
       "      <td>[[(trump, 0.548), (clinton, 0.417), (percent, ...</td>\n",
       "      <td>[[(trump, 0.548), (campaign, 0.146), (donald, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atlantic</th>\n",
       "      <td>[[(trump, 0.838), (president, 0.244), (clinton...</td>\n",
       "      <td>[[(trump, 0.838), (president, 0.244), (busines...</td>\n",
       "      <td>[[(trump, 0.838), (president, 0.244), (organiz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             lsa_tfidf  \\\n",
       "New York Times       [[(trump, 0.586), (mr trump, 0.366), (ms, 0.10...   \n",
       "New York Post        [[(trump, 0.193), (new, 0.25), (says, 0.185), ...   \n",
       "National Review      [[(trump, 0.6), (clinton, 0.171), (obama, 0.14...   \n",
       "Reuters              [[(trump, 0.422), (percent, 0.217), (billion, ...   \n",
       "NPR                  [[(trump, 0.361), (said, 0.322), (president, 0...   \n",
       "Business Insider     [[(trump, 0.414), (said, 0.393), (president, 0...   \n",
       "Talking Points Memo  [[(trump, 0.692), (president, 0.206), (house, ...   \n",
       "CNN                  [[(trump, 0.609), (president, 0.186), (people,...   \n",
       "Buzzfeed News        [[(trump, 0.212), (like, 0.253), (buzzfeed, 0....   \n",
       "Guardian             [[(trump, 0.415), (like, 0.192), (new, 0.176),...   \n",
       "Washington Post      [[(trump, 0.601), (clinton, 0.117), (president...   \n",
       "Vox                  [[(trump, 0.625), (clinton, 0.145), (president...   \n",
       "Breitbart            [[(trump, 0.633), (clinton, 0.242), (president...   \n",
       "Fox News             [[(trump, 0.548), (clinton, 0.417), (campaign,...   \n",
       "Atlantic             [[(trump, 0.838), (president, 0.244), (clinton...   \n",
       "\n",
       "                                                                lsa_cv  \\\n",
       "New York Times       [[(trump, 0.586), (mr trump, 0.366), (presiden...   \n",
       "New York Post        [[(new, 0.25), (like, 0.236), (just, 0.229), (...   \n",
       "National Review      [[(trump, 0.6), (clinton, 0.171), (president, ...   \n",
       "Reuters              [[(trump, 0.422), (percent, 0.217), (president...   \n",
       "NPR                  [[(trump, 0.361), (said, 0.322), (president, 0...   \n",
       "Business Insider     [[(trump, 0.414), (said, 0.393), (people, 0.26...   \n",
       "Talking Points Memo  [[(trump, 0.692), (president, 0.206), (people,...   \n",
       "CNN                  [[(trump, 0.609), (people, 0.197), (president,...   \n",
       "Buzzfeed News        [[(like, 0.253), (trump, 0.212), (just, 0.196)...   \n",
       "Guardian             [[(trump, 0.415), (like, 0.192), (just, 0.179)...   \n",
       "Washington Post      [[(trump, 0.601), (president, 0.174), (know, 0...   \n",
       "Vox                  [[(trump, 0.625), (clinton, 0.145), (president...   \n",
       "Breitbart            [[(trump, 0.633), (says, 0.457), (clinton, 0.2...   \n",
       "Fox News             [[(trump, 0.548), (clinton, 0.417), (percent, ...   \n",
       "Atlantic             [[(trump, 0.838), (president, 0.244), (busines...   \n",
       "\n",
       "                                                                nmf_cv  \n",
       "New York Times       [[(trump, 0.586), (mr trump, 0.366), (presiden...  \n",
       "New York Post        [[(game, 0.09), (season, 0.081), (team, 0.082)...  \n",
       "National Review      [[(trump, 0.6), (donald, 0.088), (donald trump...  \n",
       "Reuters              [[(trump, 0.422), (president, 0.188), (campaig...  \n",
       "NPR                  [[(think, 0.144), (know, 0.126), (going, 0.135...  \n",
       "Business Insider     [[(people, 0.265), (think, 0.17), (like, 0.213...  \n",
       "Talking Points Memo  [[(trump, 0.692), (donald, 0.087), (donald tru...  \n",
       "CNN                  [[(trump, 0.609), (campaign, 0.11), (donald, 0...  \n",
       "Buzzfeed News        [[(like, 0.253), (just, 0.196), (says, 0.093),...  \n",
       "Guardian             [[(like, 0.192), (just, 0.179), (don, 0.114), ...  \n",
       "Washington Post      [[(trump, 0.601), (donald, 0.079), (donald tru...  \n",
       "Vox                  [[(trump, 0.625), (campaign, 0.1), (donald, 0....  \n",
       "Breitbart            [[(trump, 0.633), (donald, 0.094), (donald tru...  \n",
       "Fox News             [[(trump, 0.548), (campaign, 0.146), (donald, ...  \n",
       "Atlantic             [[(trump, 0.838), (president, 0.244), (organiz...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_lsa_cv(topic_pub_dict):\n",
    "    def flatten(list_of_lists):\n",
    "        return [k for sublist in list_of_lists for k in sublist]\n",
    "    \n",
    "    all_lsa_cv = []\n",
    "    for key in topic_pub_dict.keys():\n",
    "        pub_topics = topic_pub_dict[key][1]\n",
    "        pub_topics = flatten([k for k in pub_topics])\n",
    "        topics = [k[0] for k in pub_topics]\n",
    "        all_lsa_cv += topics\n",
    "        \n",
    "    return all_lsa_cv\n",
    "\n",
    "\n",
    "all_lsa_cv = get_all_lsa_cv(topic_pub_dict)\n",
    "\n",
    "def get_lsa_cv_word_freq(all_lsa_cv):\n",
    "    word_freq = []\n",
    "    for s in set(articles.score):\n",
    "        sub_articles_train = articles_train[articles_train.score == s]\n",
    "        all_content = \" \".join(sub_articles_train.content)\n",
    "        for term in all_lsa_cv:\n",
    "            word_freq.append((term, s, all_content.count(term)))\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "lsa_cv_word_freq = get_lsa_cv_word_freq(all_lsa_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"lsa_cv_word_freq.pickle\", \"wb\") as f:\n",
    "    pickle.dump(lsa_cv_word_freq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21839</th>\n",
       "      <td>75248</td>\n",
       "      <td>115206</td>\n",
       "      <td>Round Two for the Supreme Court?</td>\n",
       "      <td>National Review</td>\n",
       "      <td>John Fund</td>\n",
       "      <td>2017-02-03</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>http://www.nationalreview.com/article/444571/d...</td>\n",
       "      <td>Donald Trump is known for his bluster and brag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19703</th>\n",
       "      <td>123165</td>\n",
       "      <td>183516</td>\n",
       "      <td>Japan’s Universal accuses founder Okada of imp...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Nathan Layne</td>\n",
       "      <td>2017-06-09</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>http://www.reuters.com/article/us-universal-en...</td>\n",
       "      <td>In a statement on Universal’s website issued u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15632</th>\n",
       "      <td>119094</td>\n",
       "      <td>175150</td>\n",
       "      <td>In New York, Activists Prepare Bystanders To T...</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Hansi Lo Wang</td>\n",
       "      <td>2016-12-22</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>http://www.npr.org/2016/12/22/506583208/in-new...</td>\n",
       "      <td>If you were to witness a   attack or a hate cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35427</th>\n",
       "      <td>138889</td>\n",
       "      <td>208982</td>\n",
       "      <td>FBI tries to figure out what San Bernardino at...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Mark Berman</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://web.archive.org/web/20160106000201/htt...</td>\n",
       "      <td>The   SUV involved in the police shootout w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34439</th>\n",
       "      <td>137901</td>\n",
       "      <td>207732</td>\n",
       "      <td>The definitive book about the Trump administra...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Richard Cohen</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://web.archive.org/web/20170523000535/htt...</td>\n",
       "      <td>Back in 1951, Herman Wouk published the defin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0      id                                              title  \\\n",
       "21839       75248  115206                   Round Two for the Supreme Court?   \n",
       "19703      123165  183516  Japan’s Universal accuses founder Okada of imp...   \n",
       "15632      119094  175150  In New York, Activists Prepare Bystanders To T...   \n",
       "35427      138889  208982  FBI tries to figure out what San Bernardino at...   \n",
       "34439      137901  207732  The definitive book about the Trump administra...   \n",
       "\n",
       "           publication         author        date    year  month  \\\n",
       "21839  National Review      John Fund  2017-02-03  2017.0    2.0   \n",
       "19703          Reuters   Nathan Layne  2017-06-09  2017.0    6.0   \n",
       "15632              NPR  Hansi Lo Wang  2016-12-22  2016.0   12.0   \n",
       "35427  Washington Post    Mark Berman  2016-01-05  2016.0    1.0   \n",
       "34439  Washington Post  Richard Cohen  2017-05-22  2017.0    5.0   \n",
       "\n",
       "                                                     url  \\\n",
       "21839  http://www.nationalreview.com/article/444571/d...   \n",
       "19703  http://www.reuters.com/article/us-universal-en...   \n",
       "15632  http://www.npr.org/2016/12/22/506583208/in-new...   \n",
       "35427  https://web.archive.org/web/20160106000201/htt...   \n",
       "34439  https://web.archive.org/web/20170523000535/htt...   \n",
       "\n",
       "                                                 content  \n",
       "21839  Donald Trump is known for his bluster and brag...  \n",
       "19703  In a statement on Universal’s website issued u...  \n",
       "15632  If you were to witness a   attack or a hate cr...  \n",
       "35427     The   SUV involved in the police shootout w...  \n",
       "34439   Back in 1951, Herman Wouk published the defin...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Could / should I also compare with topic modeling for entire corpus?\n",
    "\n",
    "###  Can I make a bubble plot for each publication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    return [k for sublist in list_of_lists for k in sublist]\n",
    "\n",
    "\n",
    "def get_distinct_topics(model):\n",
    "    distinct_topics = {}\n",
    "    pub_topics = list(pub_df[model].items())\n",
    "    n = len(pub_topics)\n",
    "    for k in range(n):\n",
    "        publication = pub_topics[k][0]\n",
    "        flat_topics = flatten(pub_topics[k][1])\n",
    "        unique_flat_topics = set(flat_topics)\n",
    "        distinct_topics[publication] = unique_flat_topics\n",
    "    return distinct_topics    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distinct_topics('lsa_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Are average sentence length or average article length indicative of political opinion?\n",
    "\n",
    "###  Calculate average sentence count / article\n",
    "###  Calculate average words / article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_sentences(article):\n",
    "    stoppers = [\".\", \"!\", \"?\"]\n",
    "    num_sentences = 0\n",
    "    for k in stoppers:\n",
    "        num_sentences += article.count(k)\n",
    "    return num_sentences\n",
    "        \n",
    "        \n",
    "def get_num_words(article):\n",
    "    return len(article.split(\" \"))\n",
    "\n",
    "\n",
    "def get_avg_word_length(article):\n",
    "    fillers = list(\".,!?:'-()/\")\n",
    "    for k in fillers:\n",
    "        article = article.replace(k, \"\")\n",
    "    article = article.replace('\"', '')    \n",
    "    all_words = article.split(\" \")\n",
    "    avg_word_length = np.average([len(k) for k in all_words])\n",
    "    avg_word_length = np.round(avg_word_length, 1)\n",
    "    return avg_word_length\n",
    "    \n",
    "    \n",
    "def get_adjective_count(article):\n",
    "    data = nltk.word_tokenize(article)\n",
    "    categories = nltk.pos_tag(data)\n",
    "    return len([k[1] for k in example if \"JJ\" in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_words_sentences_dict = {}\n",
    "pub_dict = get_pub_dict()\n",
    "\n",
    "for pub in publications:\n",
    "    pub_articles = [k[2] for k in pub_dict[pub]]\n",
    "    \n",
    "    avg_sentences = np.average([get_num_sentences(k) for k in pub_articles])\n",
    "    avg_sentences = np.round(avg_sentences, 1)\n",
    "    \n",
    "    avg_words = np.average([get_num_words(k) for k in pub_articles])\n",
    "    avg_words = np.round(avg_words, 1)\n",
    "    \n",
    "    avg_word_length = np.average([get_avg_word_length(k) for k in pub_articles])\n",
    "    avg_word_length = np.round(avg_word_length, 1)\n",
    "    \n",
    "    avg_words_sentences_dict[pub] = (avg_sentences, avg_words, avg_word_length)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Reuters': (37.0, 694.1, 4.7),\n",
       " 'Guardian': (45.1, 936.9, 4.6),\n",
       " 'Vox': (65.5, 1445.3, 4.4),\n",
       " 'Washington Post': (59.5, 1081.1, 4.5),\n",
       " 'Fox News': (30.4, 539.4, 4.6),\n",
       " 'New York Times': (76.0, 1192.9, 4.6),\n",
       " 'CNN': (39.5, 743.9, 4.6),\n",
       " 'NPR': (46.8, 799.5, 4.6),\n",
       " 'National Review': (50.2, 977.2, 4.7),\n",
       " 'New York Post': (25.0, 464.0, 4.4),\n",
       " 'Buzzfeed News': (44.7, 917.5, 4.6),\n",
       " 'Atlantic': (69.2, 1370.8, 4.6),\n",
       " 'Talking Points Memo': (21.0, 377.4, 4.7),\n",
       " 'Breitbart': (27.2, 525.9, 4.7),\n",
       " 'Business Insider': (23.8, 533.4, 3.9)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_words_sentences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
       "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
       "2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "\n",
       "      publication                         author        date    year  month  \\\n",
       "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
       "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
       "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
       "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
       "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
       "\n",
       "   url                                            content  score  \n",
       "0  NaN  WASHINGTON  —   Congressional Republicans have...     -1  \n",
       "1  NaN  After the bullet shells get counted, the blood...     -1  \n",
       "2  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...     -1  \n",
       "3  NaN  Death may be the great equalizer, but it isn’t...     -1  \n",
       "4  NaN  SEOUL, South Korea  —   North Korea’s leader, ...     -1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_dict = get_pub_dict()\n",
    "sentiment_polarity_dict = {}\n",
    "\n",
    "for pub in pub_dict:\n",
    "    sub_articles = articles_train[articles_train.publication == pub]\n",
    "    sub_content = list(sub_articles.content)\n",
    "    sub_sentiment = [TextBlob(k).sentiment for k in sub_content]\n",
    "    avg_polarity = np.average([k[0] for k in sub_sentiment])\n",
    "    avg_polarity = np.round(avg_polarity, 3)\n",
    "    avg_subjectivity = np.average([k[1] for k in sub_sentiment])\n",
    "    avg_subjectivity = np.round(avg_sentiment, 3)\n",
    "    subjectivity_polarity_dict[pub] = (avg_polarity, avg_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_subjectivity_polarity_dict = {}\n",
    "for pub in pub_dict:\n",
    "    pub_entries = list(pub_dict[pub])\n",
    "    pub_titles = [k[1] for k in pub_entries]\n",
    "\n",
    "    title_subjectivitysentiment = [TextBlob(k).sentiment for k in pub_titles if type(k) == str]\n",
    "    avg_polarity = np.average([k[0] for k in title_sentiment])\n",
    "    avg_polarity = np.round(avg_polarity, 3)\n",
    "    avg_subjectivity = np.average([k[1] for k in title_sentiment])\n",
    "    avg_subjectivity = np.round(avg_sentiment, 3)\n",
    "    title_subjectivity_polarity_dict[pub] = (avg_polarity, avg_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "uci_title_subjectivity_polarity = []\n",
    "\n",
    "uci_titles = list(uci.title)\n",
    "for title in uci_titles:\n",
    "    uci_title_subjectivity_polarity.append(TextBlob(title).sentiment)\n",
    "\n",
    "\n",
    "subjectivity = [k[0] for k in uci_title_sentiment_polarity]\n",
    "polarity = [k[1] for k in uci_title_sentiment_polarity]\n",
    "\n",
    "avg_uci_subjectivity = np.average(sentiment)\n",
    "avg_uci_polarity = np.average(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Breitbart': (0.006, 0.233),\n",
       " 'CNN': (0.019, 0.203),\n",
       " 'Atlantic': (0.023, 0.188),\n",
       " 'Vox': (0.05, 0.322),\n",
       " 'Buzzfeed News': (0.026, 0.251),\n",
       " 'National Review': (0.016, 0.193),\n",
       " 'Reuters': (0.022, 0.196),\n",
       " 'Talking Points Memo': (0.03, 0.23),\n",
       " 'NPR': (0.048, 0.254),\n",
       " 'New York Post': (0.02, 0.272),\n",
       " 'Washington Post': (0.022, 0.284),\n",
       " 'Guardian': (0.033, 0.261),\n",
       " 'New York Times': (0.111, 0.443),\n",
       " 'Fox News': (0.02, 0.22),\n",
       " 'Business Insider': (0.067, 0.344)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_sentiment_polarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04286047509439413"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_uci_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2252681737036944"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_uci_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business Insider': (0.086, 0.418),\n",
       " 'CNN': (0.077, 0.414),\n",
       " 'Fox News': (0.077, 0.421),\n",
       " 'Vox': (0.095, 0.452),\n",
       " 'Talking Points Memo': (0.074, 0.413),\n",
       " 'Buzzfeed News': (0.074, 0.42),\n",
       " 'New York Times': (0.08, 0.414),\n",
       " 'National Review': (0.082, 0.446),\n",
       " 'Guardian': (0.079, 0.43),\n",
       " 'Atlantic': (0.092, 0.427),\n",
       " 'NPR': (0.095, 0.43),\n",
       " 'Breitbart': (0.066, 0.419),\n",
       " 'Washington Post': (0.079, 0.426),\n",
       " 'New York Post': (0.078, 0.429),\n",
       " 'Reuters': (0.06, 0.382)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_polarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "\n",
    "def get_adj_percentage(text):\n",
    "    doc = nlp(text)\n",
    "    adj_subset = [k for k in doc if k.pos_ == \"ADJ\"]\n",
    "    num_adjs = len(adj_subset)\n",
    "    num_tokens = len(doc)\n",
    "    perc = (num_adjs / num_tokens)\n",
    "    return np.round(perc, 3) * 100\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pub_dict = get_pub_dict()\n",
    "adj_pub_count = {}\n",
    "\n",
    "for pub in pub_dict:\n",
    "    adj_count = 0\n",
    "    pub_articles = articles[articles.publication == pub]\n",
    "    pub_articles = list(pub_articles.content)\n",
    "    random.shuffle(pub_articles)\n",
    "    #  Picking a subset should be fine\n",
    "    sub_pub_articles = pub_articles[:100]\n",
    "    avg_adjs = np.average([get_adj_percentage(k) for k in sub_pub_articles])\n",
    "    adj_pub_count[pub] = avg_adjs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NPR': 7.655999999999999,\n",
       " 'National Review': 9.21,\n",
       " 'Washington Post': 7.899,\n",
       " 'Talking Points Memo': 6.824,\n",
       " 'Vox': 8.654,\n",
       " 'Business Insider': 6.657,\n",
       " 'CNN': 7.265,\n",
       " 'Buzzfeed News': 7.134999999999999,\n",
       " 'Fox News': 7.206,\n",
       " 'New York Times': 7.837999999999999,\n",
       " 'Reuters': 7.735,\n",
       " 'New York Post': 7.404,\n",
       " 'Breitbart': 7.086,\n",
       " 'Guardian': 8.034999999999998,\n",
       " 'Atlantic': 8.405999999999999}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_pub_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
